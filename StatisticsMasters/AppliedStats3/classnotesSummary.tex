% Created 2020-01-13 Mon 20:15
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Dustin Leatherman}
\date{\today}
\title{Applied Statistics 3 Summary}
\hypersetup{
 pdfauthor={Dustin Leatherman},
 pdftitle={Applied Statistics 3 Summary},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.2.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Topics}
\label{sec:orgb661e38}
\begin{itemize}
\item Multivariate Analysis

\begin{itemize}
\item Reduction Methods

\begin{itemize}
\item Principle Component Analysis (PCA)

\begin{itemize}
\item Goal
\item Usage
\item Assumptions
\item Determining Coefficients
\item Properties
\item Retaining Components
\item Interpretation
\end{itemize}

\item Canonical Correlation Analysis

\begin{itemize}
\item Goal
\item Usage
\item Assumptions
\item Intepretation
\end{itemize}
\end{itemize}

\item Discriminant Function Analysis

\begin{itemize}
\item Goal
\item Usage
\item Assumptions
\item Determining Coefficients
\item Tests of Significance
\end{itemize}

\item Classification Analysis

\begin{itemize}
\item Assumptions
\item Usage
\item Error Rates

\begin{itemize}
\item Estimation
\end{itemize}

\item Nearest Neighbor Classification Rule (KNN)
\end{itemize}

\item Cluster Analysis

\begin{itemize}
\item Goal
\item Usage
\item Assumptions
\item Types

\begin{itemize}
\item Hierarchical

\begin{itemize}
\item Linkage Methods
\item Standardization
\end{itemize}

\item Partitioning

\begin{itemize}
\item K-means Clustering
\item Wards method
\item K-Medoids
\end{itemize}

\item Model-based
\end{itemize}
\end{itemize}

\item Multi-Dimensional Scaling

\begin{itemize}
\item Classical MD Scaling

\begin{itemize}
\item Goal
\end{itemize}
\end{itemize}

\item Correspondence Analysis

\begin{itemize}
\item Interpretation
\end{itemize}
\end{itemize}

\item Odds \& Odds Ratio

\begin{itemize}
\item properties
\item Sampling Distribution of the Log-estimated Odds Ratio
\end{itemize}

\item 2 X 2 Counts

\begin{itemize}
\item Tests of Homogeneity
\item Tests of Independence
\item Sampling Schemes
\end{itemize}

\item Chi-squared Test

\begin{itemize}
\item Mantel-Haenszel
\item R x C
\end{itemize}

\item Fisher's Exact Test

\begin{itemize}
\item Mantel-Haenszel Excess

\begin{itemize}
\item Assumptions
\end{itemize}
\end{itemize}

\item Generalized Linear Models

\begin{itemize}
\item Types
\end{itemize}

\item Logistic Regression

\begin{itemize}
\item Maximum Likelihood Estimation (MLE)

\begin{itemize}
\item Properties
\end{itemize}

\item Likelihood Ratio Test / Drop-in-Deviance Test
\item Probit Regression
\item Model Assessment
\item Binomial Responses

\begin{itemize}
\item Model Assessment
\item Extra Binomial Variation (Over dispersion)

\begin{itemize}
\item Determining existence
\item Estimating \(\psi\)
\end{itemize}
\end{itemize}

\item Multilevel Categorical Responses

\begin{itemize}
\item Ordinal Categorical Responses
\end{itemize}
\end{itemize}

\item Log-Linear Models (Poisson Regression)

\begin{itemize}
\item Characteristics
\item Interpretation
\item Model Assessment
\item Extra Poisson Variation (Over dispersion)

\begin{itemize}
\item Checking for Over dispersion
\end{itemize}
\end{itemize}

\item Negative Binomial Regression
\item Experiment Design

\begin{itemize}
\item Studies

\begin{itemize}
\item Prospective
\item Retrospective
\item Matched Case-Control studies
\end{itemize}

\item Research Design Tool Kit

\begin{itemize}
\item Improving Confidence Intervals
\item Choosing a Sample Size

\begin{itemize}
\item Studies comparing 2 proportions
\end{itemize}
\end{itemize}

\item Designing a Study
\item Factorial Design

\begin{itemize}
\item 2$\backslash$\textsuperscript{2}
\item 2$\backslash$\textsuperscript{3}
\item 2$\backslash$\textsuperscript{k}
\end{itemize}
\end{itemize}
\end{itemize}

\section{Multivariate Analysis}
\label{sec:org18f8328}
\subsection{Reduction Methods}
\label{sec:org5225517}
\subsubsection{Principle Component Analysis (PCA)}
\label{sec:orgde1af73}
\begin{enumerate}
\item Goal
\label{sec:orgd8f213c}
\begin{itemize}
\item Variable and/or Data Reduction
\item Create a few linear combos which retain a large amount of the
variance
\end{itemize}

\item Usage
\label{sec:org3f0e7a1}
These principle component combinations can be used in subsequent
analysis as explanatory variables.

\item Assumptions
\label{sec:orgd9b9aca}
\begin{itemize}
\item Linearity
\item Some Correlation among factors
\end{itemize}

\item Properties
\label{sec:org33b9b90}
Let a given principle component be represented by \(z_j\)

\begin{itemize}
\item Ordered by variance of Z
\item Expect Most information to be contained in the first few components
\item \([z_1, z_q]\) are uncorrelated
\item \(\sum var(z_j) = \sum var(y_j)\)
\end{itemize}

\item Determining Coefficients
\label{sec:org1b08beb}
\begin{itemize}
\item \(Var(Z_1)\) maximized with constraint \(a_1^{\prime}a_1\) = 1
\item \(Var(Z_2)\) maximized with constraint \(a_2^{\prime}a_2\) = 1 and
\(cov(Z_1, Z_2)\) = 0.
\end{itemize}

To generate coefficients, use: * The original variables' covariance
matrix (if using original vars) * The original variables' correlation
matrix (if using standardized original vars)

\begin{enumerate}
\item Original vs Standardized Vars
\label{sec:orgba99276}
\textbf{Orignal} * Easier to interpret * Results dependent on unit of
measurement * Principle Components tend to reflect vars with largest
variance

\textbf{Standardized} * Can be used when vars are of difference scales * More
difficult to interpret * More common
\end{enumerate}

\item Retaining Components
\label{sec:org7169ae3}
Two options
\begin{enumerate}
\item Enough should be retained to explain 80\% of the total
\end{enumerate}
variation
\begin{enumerate}
\item Lower bound on the number of retained components
\item \(\frac{\Sigma lambda_i} {q}\) (average eigenvalue)
\begin{itemize}
\item where \(lambda_i\) are the eigenvalues of the covariance matrix (original)
or correlation matrix (standard)
\item avg eignevalue = 1 when using the correlation matrix
\end{itemize}
\item Scree plot
\begin{enumerate}
\item Plots eigenvalues vs the component \#
\item Choose \# of components where the plot begins to \emph{flatten} out
\end{enumerate}
\end{enumerate}

\item Interpretation
\label{sec:org2698099}
Each Principle Component describes how a group of variables are
interrelated.

\begin{itemize}
\item Focus on \emph{loadings} (coefficients). Loading > 0.5 helps determine
which vars are influential
\item If all elements of the first eigenvector/coefficients/loadings are
positive, Principle Component measures \emph{size}
\item If some are positive and negative, then the PC measures a difference
of the variables
\item If all loadings are roughly the same size in magnitude and the same
sign, the PC can be interpreted as a weighted average
\end{itemize}
\end{enumerate}

\subsubsection{Canonical Correlation Analysis}
\label{sec:orgd7027c9}
Start with two sets of variables: 1. A set of response variables - y1,
y2, \ldots{}, yq 2. A set of explanatory variables - x1, x2, \ldots{}, xq

\begin{enumerate}
\item Goal
\label{sec:org758b7a6}
$$
    U = \sum_{i = 1}^q a_i y_i
$$

$$
   V = \sum_{i = 1}^p b_i x_i
$$

Find coefficients a1, a2, \ldots{}, aq and b1 , b2, \ldots{}, bq that maximize the
correlation between U and V.

These are called canonical variates which are essentially linear
combinations of the original two sets of variables.

\textbf{Number of Canonical correlations}: \(s = min(p, q)\)

\begin{itemize}
\item \(U_i\) and \(V_i\) are correlated. i = [1, s]
\item \(U_i\) and \(V_j\) are uncorrelated. i \(\ne\) j
\item \(V_i\) and \(V_j\) are uncorrelated. i \(\ne\) j
\item \(U_i\) and \(U_j\) are uncorrelated. i \(\ne\) j
\end{itemize}

Canonical Correlation Sqaured: \([r_1^2, r_s^2]\) * Proportion of variance
explained in the dependent vars (Y's) explained by the independent set
of vars (X's) along a given dimension (s dimentions)

\textbf{Redundancy Analysis}: Explains variation by evaluating the adequacy of
prediction from the canonical analysis,

\item Usage
\label{sec:org7a6024d}
\begin{itemize}
\item Measure correlation between X's and Y's
\item Extension of multiple correlation (\(\sqrt{R^2}\))
\item Often a compliment to multivariate regression
\end{itemize}

\begin{enumerate}
\item When to use
\label{sec:orga8c03c0}
\begin{itemize}
\item Regression analysis appropriate but more than one dependent variable
Y
\item Useful when dependent variables are moderately inter-related
\item Can be used to test independence between the independent vars (X's)
and dependent vars (Y's)
\end{itemize}
\end{enumerate}

\item Assumptions
\label{sec:orgc3d04d1}
\begin{itemize}
\item Linearity of Correlations
\item Linearity of Relationships
\item Multivariate Normality

\begin{itemize}
\item Desirable since it standardizes a distribution to allow for a
higher correlation among variables
\item Highly recommended that all vars are evaluated for normality and
transformed if needed
\end{itemize}
\end{itemize}

\item Intepretation
\label{sec:orgb5e2da0}
Low p-values indicate significance of a correlation. In an example with
4 canonical correlations:
\begin{itemize}
\item CV\textsubscript{1}: \$H\textsubscript{0}: \(\rho\)\textsubscript{1} = \(\rho\)\textsubscript{2} = \(\rho\)\textsubscript{3} = \(\rho\)\textsubscript{4} = 0
\item CV\textsubscript{2}: \(H_0: \rho_2 = \rho_3 = \rho_4 = 0\)
\item CV\textsubscript{3}: \(H_0: \rho_3 = \rho_4 = 0\)
\item CV\textsubscript{4}: \(H_0: \rho_4 = 0\)
\end{itemize}

\item Discriminant Function Analysis (DFA)
\label{sec:orge4c233d}
\begin{enumerate}
\item Goal
\label{sec:org69f3983}
Classify a subject or unit into two or more groups based on info
collected on independent variables. Groups \textbf{must} be clearly defined.

How likely is a subject in \({group}_j\) based on the basis of a set of
quantitative variables?

\item Usage
\label{sec:orga1c81d5}
Come up with a single set of coefficients to apply to all groups \textbf{then}
Construct linear combinations of these variables and use them to
distinguish populations.

Distribution between groups? * Yes: parametric methods (linear or
quadratic DFA ) * No: non-parametric method

\item Assumptions
\label{sec:org4b5bc76}
\begin{itemize}
\item Equal Spread
\item Some Assume Normality
\end{itemize}

\item Determining Coefficients
\label{sec:org4e0300e}
\begin{itemize}
\item Maximize separation between two groups \textbf{Mahalanobis distance}
\end{itemize}

$$
D^2 = \frac{(\bar{z_1} - \bar{z_2})^2} {s^2_z} = (\bar{y_1} - \bar{y_2})^TS_{pl}^{-1}(\bar{y_1} - \bar{y_2}) 
$$

Multi-dimensional generalization of measuring how many std devs away
from a point is the mean (or centroid) of the distribution (Like a
Z-score).

Scalings from LDF are not the same as
\(A^T = S_{pl}^{-1}(\bar{y_1} - \bar{y_2})\)
\end{enumerate}

\item K Groups
\label{sec:org7545a45}
\begin{enumerate}
\item Goal
\label{sec:org37cacda}
Find a vector \(A\) that maximizes separation between
\([\bar{z_1},\bar{z_k}]\)

\item Usage
\label{sec:orgf693a13}
\textbf{How?}
\begin{itemize}
\item Replace \((\bar{y_1} - \bar{y_2})^T\) with the \(H\) matrix from
\end{itemize}
MANOVA
\begin{itemize}
\item Replace \(S_{pl}\) with \(E\) matrix
\end{itemize}

H indicates spread between groups E indicates spread within each group

$$
    \lambda = \frac{a^T H a}{a^T E a} \newline 
    \to a^T(Ha - \lambda Ea) = 0 \newline 
    \to (E^{-1}H - \lambda I)a = 0
$$

Solutions are the eigenvalues \([\lambda_1,\lambda_s]\) and eigenvectors
\([a_1, a_s]\) of \(E^{-1}H\) where \(s = rank(H) = min(k-1, s)\)

From eigenvectors \([a_1, a_s]\) of \(E^{-1}H\), s \textbf{discriminant functions}
are obtained:
\begin{itemize}
\item \(z_1 = a^Ty\)
\item \(z_2 = a^Ty\)
\ldots{}
\item \(z_s = a^Ty\)
\end{itemize}

These discriminant functions are uncorrelated. They show the dimensions
or directions of differences among \([y_1, y_k]\). The relative importance
of each discriminant function can be assessed by considering its
eigenvalue as a proportion of the total. $$
\frac{\lambda_i}{\sum_{j = 1}^s\lambda_j}
$$

\textbf{Matrix \(E^{-1}H\) is not symmetric.} Special computation must be done in
R:
\begin{itemize}
\item Find matrix U that is the Cholesky factorization of E. \(E = U^TU\)
\item Find the eigenvector b of the matrix \((U^{-1})^THU^{-1}\)
\item \(a = U^{-1}b\) is an eigenvector of \(E^{-1}H\)
\end{itemize}
\end{enumerate}

\item Tests of Significance
\label{sec:org0000c01}
\begin{itemize}
\item Two Group Case

\begin{itemize}
\item Use Hotellings \(T^2\) to test \(H_0: a = 0\)
\end{itemize}

\item K Group Case

\begin{itemize}
\item Wilks' lambda since eigenvalues are the same as eigenvalues from
MANOVA
\item \(V_m = [N - 1 - \frac{1}{2}(p + k)]\sum_{i = 0}^k log(1 + \lambda_i)\)

\begin{itemize}
\item p = \# of vars
\item k = \# of groups
\item \(V_m \approx \chi^2\) (p - m + 1)(k - m) degrees of freedom
\end{itemize}
\end{itemize}
\end{itemize}

Forward, Backward, or Stepwise Selection can be performed to determine
predictors that are most significant for discriminating against others.

\item Interpretation
\label{sec:org9d38876}
Standardizing helps. The largest magnitude contributes most to the
equation (similar to PCA and CCA).
\end{enumerate}

\subsubsection{Classification Analysis}
\label{sec:org75799c5}
The predictive aspect of Discriminant Analysis. Synonyms include
Discriminant Analysis, Pattern Recognition, and Cluster Analysis.

\begin{enumerate}
\item Assumptions
\label{sec:org9cadce6}
\begin{itemize}
\item No assumptions around distributions
\item \(\Sigma_1 = \Sigma_2\) (Equal covariance matrices)

\begin{itemize}
\item If violated, Distance function is
\(D_i^2(y) = (y - \bar{y_i})^TS_i^{-1}(y - \bar{y_i})\) where \(S_i\)
is the sample covariance for the \(i^{th}\) group.
\end{itemize}
\end{itemize}

\item Usage
\label{sec:org659d842}
\begin{enumerate}
\item Obtain a sample of observation vectors from each group
\item Choose a Sampling Unit whose group membership is unknown
\item Assign unit to a group based on vector of p measure values (y)
associated with the unit*
\end{enumerate}

\textbf{If prior probabilities \(p_1\) and \(p_2\) are known for 2 populations, the classification rule can be modified.}

Normal Base Classification Rule
\begin{itemize}
\item \(f(y | G_1) \approx N_p(\mu_1, \sigma)\)
\item \(f(y | G_2) \approx N_p(\mu_2, \sigma)\)
\end{itemize}

\item Error Rates
\label{sec:orgb2c8c72}
\textbf{Error Rate}: probability of misclassification Correct Classification
\textbf{Rate}: Complement of Error Rate

\begin{enumerate}
\item Estimation
\label{sec:org8f053bc}
A simple method is to plug the values back in and see how many matched.
For large samples, the error rate has a small amount of bias for
estimating the actual error rate.

For small samples, \textbf{Holdout/leave-one-out/Cross Validation}. All but one
observations used to compute the classification rule then used to
classify the omitted observation
\end{enumerate}

\item Nearest Neighbor Classification Rule (KNN)
\label{sec:org58e7584}
Compute distance from \(y_i\) to all other points \(y_i\) using distance
function
$$
 (y_i - y_j)^TS_{pl}^{-1}(y_i - y_j), j \ne i
$$

If a majority of K points belong to \(G_1\), assign \(y_i\) to \(G_1\), else
\(G_2\)

\textbf{Choosing K} * \(K = \sqrt{n_i}\) * Try several values of k and use the
one with the best error rate
\end{enumerate}

\subsubsection{Cluster Analysis}
\label{sec:org62a4e83}
\begin{enumerate}
\item Goal
\label{sec:org0b28829}
Separate Individual observations/items into groups/clusters on the basis
of values for p variables measured on each variable

\begin{itemize}
\item items/objects == rows
\item Distance measured is typically Euclidean
\end{itemize}

Type of unsupervised classification because the nature or number of
groups is not necessarily known prior to classification

\item Usage
\label{sec:org1ed5d7a}

\item Assumptions
\label{sec:org6d03fd2}
\begin{itemize}
\item N objects/cases/rows of data
\item K clusters/groups

\begin{itemize}
\item If K is known, the number of ways to partition N into K is a
\textbf{stirling number of the second kind}
\item If K is not known, the number of possible partitions is much
larger
\end{itemize}
\end{itemize}

\item Types
\label{sec:org09d23f3}
\begin{enumerate}
\item Hierarchical
\label{sec:org25f2dba}
Cluster data in a series of n steps, joining observations together
step-by-step to form clusters.

\begin{itemize}
\item Fast computation for small datasets
\item Dendogram for visualizing a variety of k clusters
\end{itemize}

\begin{enumerate}
\item Linkage Methods
\label{sec:orgdea0e9d}
\textbf{Single Linkage or Nearest Neighbor} Join clusters whose min distance
between object is smallest $$
D_{AB} = min(d_{ij})
$$

where \(d_{ij}\) is the distance between an element in A and B.

\textbf{Complete Linkage}: Single Linkage with max instead of min. \textbf{Average
Linkage}: Single Linkage with avg instead of min.

\item Standardization
\label{sec:orgc1a0943}
\begin{itemize}
\item Divide each column by its sample std dev so all variables have a std
dev of 1
\item Divide each variable by its sample range
\item Z Scores
\end{itemize}
\end{enumerate}

\item Partitioning
\label{sec:org6a2c3d9}
For a fixed value of K, seek the best possible partition for that K
which optimizes some objective function.

\begin{enumerate}
\item K-means Clustering
\label{sec:org9079c92}
Find the partition of N objects into K clusters that minimize
within-cluster SS. Traditionally, distance between clusters is
euclidean. Goals is to minimize the sum of squared euclidean distances

$$
WSS = \sum_{c = 0}^k \sum_{i = 0}^n d^2_{E(y_i, \bar{y_c})}
$$

Final clustering result dependent on initial configuration of rows. Good
to rerun the algorithm a few times with different starting points to
ensure stable results.

\item Wards method
\label{sec:orgd7b2c6f}
Mix of Hierarchical and K-means. Each object starts as its own cluster
and concludes with all objects in one cluster. At each step, the method
searches all possible ways to join a pair of clusters so that the WSS is
minimized for that step.

\item K-Medoids
\label{sec:orgeffda69}
Robust alternative to K-Means. Minimizes

$$
C_{md} = \sum_{c = 0}^k \sum_{i = 0}^n d(y_i, m_c)
$$

\(M_c\) is a medoid (most representative object). Best to think of it as a
p-variate median. Like K-Means, K-Medoids does not globally minimize its
criterion in general.

\textbf{Pros}
\begin{itemize}
\item Accepts a dissimilarity matrix as well as raw data matrix
\item Generates silhouttes for K-clusters so don't need to decide K ahead of
\end{itemize}
time

\textbf{Cons}
\begin{itemize}
\item Computationally infeasible for n > 5000.
\end{itemize}

Other criteria for choosing k include the Dunn Index and the
Davies-Bouldin Index
\end{enumerate}

\item Model-based
\label{sec:orgc6ae294}
Assumes an underlying distribution for the K clusters.
\end{enumerate}
\end{enumerate}

\subsubsection{Multi-Dimensional Scaling}
\label{sec:orgf17bf45}
Use distances to measure how different multivariate observations were
from each other. Can take a multivariate dataset (a set of p-dimensional
vectors) and calculate distances between pairs of vectors.

\begin{enumerate}
\item Classical MD Scaling
\label{sec:org05b515e}
\begin{enumerate}
\item Goal
\label{sec:org62b7cbb}
Given an N x N matrix, construct a map containing multivariate points.
There are no unique or best solutions where to place points on map.

Sometimes referred to as Principle Coordinates Analysis.
\end{enumerate}
\end{enumerate}

\subsubsection{Correspondence Analysis}
\label{sec:orgdf25703}
Contingency Table presents sample values for two categorical variables
and test for independence between the two. This supplements a chi-square
test

\textbf{Chi-square distance}: Column Proportions with entries
\(p_{ij} = \frac{n_{ij}}{n_i}\)

\begin{enumerate}
\item Interpretation
\label{sec:orgfaef80d}
With all rows and categories plotted:
\begin{itemize}
\item Two row categories near each other have similar conditional distributions across columns * Two column
\end{itemize}
categories have similar profiles
\begin{itemize}
\item A Row and Column Category near tend to appear more ofthen than expected under independence.
\end{itemize}
\end{enumerate}

\section{Odds \& Odds Ratio}
\label{sec:org485f9c2}
The probability of something happening (\(\omega\))

\subsection{Properties}
\label{sec:org00b3400}
\begin{itemize}
\item \(\omega \geq 0\)
\item If P (probability) = 0.5, them \(\omega = 1\) (50-50 odds)
\item If \(\omega\) is odds of success, \(\frac{1}{\omega}\) is odds of failure
\item \(P = \frac{\omega}{\omega + 1}\)
\end{itemize}

\subsection{Odds Ratio}
\label{sec:orgcc6c55a}
\textbf{Example}

$$
    \phi = \frac{\omega_1}{\omega_2} = 5 \newline 
    \to \omega_1 = 5\omega_2
$$

The odds of "success" in Group 1 is 5 times the odds of "success" in
Group 2

\(\phi = \frac{\omega_1}{\omega_2}\)

\begin{verse}
  | Response |\\
  | Yes | No |\\
1 | \(n_{11}\) | \(n_{12}\) |\\
2 | \(n_{21}\) | \(n_{22}\) |\\
\end{verse}

\(\phi = \frac{n_{11}n_{22}}{n_{21}n_{12}}\)

\subsubsection{Odds Ratios vs. Population Proportion}
\label{sec:org00212d3}
\begin{itemize}
\item \(\phi\) tends to remain more nearly constant over levels of
confounding variables
\item \(\phi\) is the only parameter that can be used to compare groups of
responses from a \textbf{retrospective} study
\item \(\phi\) extends into Logistic Regression models
\end{itemize}

\subsubsection{Sampling Distribution of the Log-estimated Odds Ratio}
\label{sec:org71c7e07}
\begin{itemize}
\item \(E(\log(\hat{\phi})) \approx \log(\phi)\)
\item \(Var(\log(\hat{\phi})) \approx \frac{1}{n_1 p_1 (1 - p_1)} + \frac{1}{n_2 p_2 (1 - p_2)}\)

\begin{itemize}
\item Similar to a binary distribution
\end{itemize}

\item if \(n_1\) and \(n_2\) are sufficiently large, the sampling distribution
is approximately normal
\end{itemize}

\section{2 X 2 Counts}
\label{sec:org75e4695}
$$
    H_0: \pi_1 - \pi_2 = 0 \equiv H_0: \frac{\omega_1}{\omega_2} = \theta = 1
$$

\subsection{Tests of Homogeneity}
\label{sec:orgfa947ce}
Is a binary response the same across multiple populations?

\subsection{Tests of Independence}
\label{sec:org663852d}
Is there an association between row and column factors without
specifying one of them as a response variable? Refers to a single
population.

\(H_0\): The row category is independent of the column category

\subsection{Sampling Schemes}
\label{sec:org22910e7}
Odds Ratio can be used with any Sampling Scheme

\subsubsection{Poisson}
\label{sec:orgb613b8b}
Frequency of success over a period of time or space. Random sample from
a single population where each member falls into a cell in an R x C
table.

\textbf{No Marginal Totals known in advance}

Used for tests of \textbf{homogeneity} and \textbf{independence}

\subsubsection{Multinomial}
\label{sec:org55b8c5a}
K categories for a sample of N. Similar to Poisson except \textbf{Total Sample
Size (T) is fixed in advance}.

Used for tests of \textbf{homogeneity} and \textbf{independence}

\subsubsection{Prospective Product Binomial}
\label{sec:orgefa5e9f}
More than one Binomial Distribution is present. Random samples selected
from each population

\textbf{Row Totals fixed in advance}

Used for Test of \textbf{homogeneity} but only for the odds ratio

\subsubsection{Retrospective Product Binomial}
\label{sec:orga62de6e}
Flip explanatory and Response variable from Prospective Binomial
Sampling

\textbf{Column totals fixed in advance}

Used for Test of \textbf{homogeneity} but only for the odds ratio

\subsubsection{Randomized Binomial Experiment}
\label{sec:orgea5627b}
Subjects randomly allocated to the two levels of the explanatory factor
(Rows of the table). This follows Prospective Product Binomial except
instead of random sampling, randomization of subjects into groups is
used.

Used for Tests of \textbf{homogeneity}

\subsubsection{Hypergeometric Probability Distribution}
\label{sec:orge387851}
If interest is stricly focused on the odds ratio, analysis may be
conducted conditionally on the row and column totals

\textbf{Both row and column totals are fixed}

Used in Fisher's Exact Test

\section{Chi-square Tests}
\label{sec:orgeca29cc}
\subsection{Pearson Chi-Square Test for Goodness of Fit}
\label{sec:orgc23d9bd}
Determine GoF based on the assumption that the expected count follows a
\(\chi^2\) distribution.

\textbf{Observed Count}: Number of units that fall into a cell. \textbf{Expected
Count}: Number of units predicted by theory to fall into a cell when
\(H_0\) is true

$$
    \chi^2 \approx \sum\frac{(Observed - Expected)^2}{Expected}
$$

If \(H_0\) is true, then the chi-square test approximates \(\chi^2\) with df
= number of cells - 1

\subsection{Chi-Squared Test of Independence in an R X C Table}
\label{sec:org45fe101}
When H0 is true, sampling distribution of \(\chi^2\) has an approximate
\(\chi^2\) distribution with (r - 1) X (c - 1) df where r is the number of
rows and c is number of columns.

\subsubsection{Limitations}
\label{sec:orgf4d3a83}
\begin{itemize}
\item Only Product is a p-value
\item No associated parameter to describe the degree of dependence

\begin{itemize}
\item look at expected ratios vs actual ratios to determine dependency
\end{itemize}

\item Alternative Hypothesis very general
\item When 3+ rows and columns involved, may be a more specific form of
dependence to explore
\end{itemize}

\subsection{Mantel-Haenszel}
\label{sec:orgbad9165}
A more powerful alternative to the Pearson Chi-square Test when at least
one of the factors are \textbf{ordinal}. An ordinal may be defined as a
midpoint for a range of response variables.

\(r\) = some measure of the sample correlation between two factors \(n\) =
sample value

\(M^2 = (n - 1)r^2\)

\(H_0: \rho = 1\) \(H_A: \rho \neq 1\)

Sampling Distribution of \(M^2 \approx \chi^2\) with df = 1 under \(H_0\)

\section{Fisher's Exact Test}
\label{sec:org90ec677}
Randomization test based on statistic \(\pi_1 - \pi_2\). When data is
observational, it can be thought of as a permutation test. This is a
useful interpretation when the entire population has been sampled or a
sample is not random. * Inference possible for Poisson, Multinomial, and
Product Binomial sampling schemes

Can be used for tests of equal population proportions, equal population
odds, or independence

\subsection{Mantel-Haenszel Excess}
\label{sec:orge36c5e6}
\textbf{Excess}: Observed Count - expected count in one cell of a R x C table.
This is like a residual for cell counts.

Excess of \(n_{11}\) = \(n_{11} - \frac{R_1 C_1}{T}\)

Under \(H_0\): * E(Excess) = 0 * Var(Excess) =
\(\frac{R_1 R_2 C_1 C_2}{TT(T - 1)}\)

For a 2 x 2 table of counts, excess is an approximation of Fisher's
Exact Test.

An overall association can be developed for a third factor. A weighted
average of the odds ratios across 2 x 2 tables should be calculated.
This treats the third factor as a block.

Tests for conditional independence and homogenous assocation for the k
conditional odds ratios in K 2 x 2 tables. It combines sample odds
ratios for the partial K tables into a single summary measure of partial
assocation.

Appropriate for prospective, retrospective observational data, and
randomized experiments.

\subsubsection{Assumptions}
\label{sec:org7d3686f}
\begin{itemize}
\item Odds Ratio same in each 2 x 2 Table. (Use Breslow-Day Statistic)

\begin{itemize}
\item \(H_0\): X and Y are conditionally independent given Z
(\(\theta_{XY(k)} = 1\))
\end{itemize}

\item Sum of expected counts over all tables should be at least 5.
\end{itemize}

\section{Generalized Linear Models}
\label{sec:org87583e4}
Probability Model in which the mean of a response variable is related to
explanatory variables through a regression equation. There is a function
out there which converts a response variable to a linear function. This
is called the \textbf{link function}.

\subsection{Types}
\label{sec:orgd45eb9c}
\textbf{Link Function}: A specified function of \(\mu\) equal to the regression
structure. The non-linearity is contained within the link function.

\(g(\mu) = \beta_0 + \sum_{i = 1}^p \beta_i X_i\)

\subsubsection{Normal}
\label{sec:orge94e9b6}
Used for Ordinary Least Squares (OLS) Regression \textbf{Link}: Identity
\textbf{Function}: \(g(\mu) = \mu\)

\subsubsection{Poisson}
\label{sec:org19e95b7}
Used to count occurrences in a fixed time or space \textbf{Link}: Log
\textbf{Function}: \(\log(\mu)\)

\subsubsection{Bernoilli, Binomial, Categorical, Multinomial (Logistic)}
\label{sec:org1351499}
Outcome of a single binary response OR number of successes OR outcome of
a single K-way occurrence
\begin{itemize}
\item \textbf{Link}: Logit
\item \textbf{Function}: \(\log(\frac{\pi}{1 - \pi}) = \log(\theta) = \beta_0 + \sum_{i = 1}^p \beta_i X_i = \eta\)
\end{itemize}

Known as the log-odds because it is a log function of the odds where the
odds of success = \(\pi\)

\section{Logistic Regression}
\label{sec:orgd6ab6cc}
\textbf{Logistic Function}: Inverse of the Logit function $$
    \pi = \frac{e^{\eta}}{1 + e^{\eta}}
$$

\begin{itemize}
\item \(E(Y) = \pi\)
\item \(Var(Y) = \pi (1 - \pi)\)
\end{itemize}

$$
    \omega = \frac{\pi}{1 - \pi} = e^{\eta} \newline
    \omega_A = e^{\beta_0 + \beta_1A} \newline
    \omega_B = e^{\beta_0 + \beta_1B} \newline
$$

Odds of A/B

$$
    \omega_{AB} = \frac{\omega_A}{\omega_B} = e^{\beta_1(A - B)}
$$

\subsection{Maximum Likelihood Estimation (MLE)}
\label{sec:org692633f}
$$
    Pr(Y = y) = \pi^y (1 - \pi)^{1 - y}
$$

Joint Probability Mass Function $$
    P(Y_1 = y_1, ...) = \prod Pr(Y_i = y_i)
$$

To find MLEs of the Logistic Regression coefficients, set each p + 1
partial derivatives to 0.

Solutions for parameters to this system of equations are MLEs for the LR
coefficients. The solution to this system does not exist in closed form;
therefore, iterational computational procedures such as the
Newton-Raphson, are used.

\subsubsection{Properties}
\label{sec:org08620d3}
If a model is correct and the sample size is large enough:
\begin{itemize}
\item MLEs are essentially unbiased
\item Formulas exist for estimating the std devs of the Sampling Distribution of the
Estimators
\item Estimators are MVUE
\item The sampling distribution is approximately Normal
\end{itemize}

When working with Asymptotic Normal Results, these procedures are called
Wald procedures. They assume large sample sizes make everything
statisically okay.

\subsection{Likelihood Ratio Test / Drop-in-Deviance Test}
\label{sec:orge7fb755}
Analogous to Extra Sum of Squares F-Test in Linear Regression. Compares
a full model to a reduced model. When \(H_0\) is true, the reduced model
is the correct model.

\(LRT \approx \chi^2(\nu)\) where \(\nu\) = diff(num$\backslash$\textsubscript{param}$\backslash$\textsubscript{full},
num$\backslash$\textsubscript{param}$\backslash$\textsubscript{reduced}). With GLMs, a quantity called Deviance is used.

LMAX = Maximum Likelihood Function

Deviance = Sum of Squared Residuals = -2 * log(LMAX)

\(LRT = 2 \log(LMAX_{full}) - 2log(LMAX_{reduced})\) = Deviance$\backslash$\textsubscript{full} -
Deviance$\backslash$\textsubscript{reduced}

To test significance of a single term, DinD test between full model and
full model minus the single term. This is not the same as Wald's test
for a single coefficient. If the two give different results, DinD has a
more reliable p-value.

\subsubsection{Model Assessment}
\label{sec:org64821db}
\begin{itemize}
\item For model terms, Informal testing of extra terms such as squared or
interaction terms is important.
\item For model adequacy

\begin{itemize}
\item Hosmer-Lemeshow GoF Test
\item Deviance Residual Plots vs Predicted Values and each of the
predictor variables

\begin{itemize}
\item Loess function should be as flat as possible
\end{itemize}

\item More complicated GoF tests exist
\end{itemize}
\end{itemize}

\textbf{AIC / BIC}

AIC = deviance + log(n) * p BIC = deviance + 2 * p

\subsection{Probit Regression}
\label{sec:orga4d2bc7}
Any cumulative distribution function \(F(\pi)\) has characteristics
similar to the logit function. Typically, \(F(\pi)\) is chosen to be the
inverse of the Normal CDF. As long as \(\pi:[0.2,0.8]\), it is similar to
logistic regression.

\subsection{Binomial Responses}
\label{sec:org36f3282}
\(Y_i \approx Bin(m_i, \pi_i)\)

\subsubsection{Model Assessment}
\label{sec:org076bccd}
\begin{itemize}
\item Scatterplots: Empirical logits vs Explanatory Variables
\begin{itemize}
\item log odds vs explanatory vars. Log-odds on Y, explanatory on X. If
it looks linear, Good! Otherwise, a transformation may be needed
\end{itemize}
\item Examining Residuals
\item Deviance GoF test
\begin{itemize}
\item DinD with intercept-only-model and proposed model
\end{itemize}
\end{itemize}

\subsubsection{Examining Residuals}
\label{sec:org9778c00}
\textbf{Deviance Residual}: sum of all n squared deviance residuals = deviance
statistic. This measures discrepency in the likelihood function to the
fit of the model at each observation.

\textbf{Pearson Residual}: Observed Binomial Response variable minus estimated
mean, divided by estimated std dev. (like Z Score). Roughly mean = 0 and
var = 1.

With at least 5 trials in any binomial response, then any residual
greater than 2 in magnitude may be a possible outlier. No discernable
pattern indicates the error terms are normal

\subsubsection{Extra Binomial Variation (Over dispersion)}
\label{sec:orgc51c1ed}
If binomial trials are not independent or important explanatory
variiables are not included in the model for \(\pi_i\), response counts
will no longer have binomial distributions

When Over dispersion is present, regression parameter estimates will not
be seriously biased but standard errors tend to be smaller leading to
small p-values, narrow C.I., and mistaken interpretations.

The \textbf{quasi-likelihood approach} assumes a relationship between mean and
Var(Y) rather than a specific probability distribution for Y. The
variance formula is multiplied by an estimated constant \(\psi\).
\(\psi \gt 1\) indicates overdispersion.

It does not affect regression coefficients but it affects standard
errors.

\begin{enumerate}
\item Determining existence
\label{sec:org07ba2c9}
Yes to any of these questions cautions the use of the binomial model.
\begin{itemize}
\item Are binary responses included in each count unlikely to be independent?
\item Are Observations with identical values in explanatory variables likely
\end{itemize}
to have different \(\pi_i\)'s?
\begin{itemize}
\item Is the model for \(\pi\) naive?
\end{itemize}

\item Estimating \(\psi\)
\label{sec:orgceef634}
$$
    \hat{\psi} = \frac{\sum Pres_i^2}{n - p}
$$

Let D = number of parameters in the full model. $$
    F = \frac{\frac{DinD}{D}}{\hat{\psi}}
$$
\end{enumerate}

\subsection{Multilevel Categorical Responses}
\label{sec:orgc6236bf}
Let \(J\) be the number of categories for \(Y\) and \([\pi_1, \pi_i]\) denote
the reponse probabilities.

Multicategory logit models simultaneously use all pairs of categories by
specifying the odds of outcome in one category instead of another.

$$
    \log(\frac{\pi_a}{\pi_b}) = \log(\frac{\frac{\pi_a}{\pi_J}}{\frac{\pi_b}{\pi_J}}) = \log(\frac{\pi_a}{\pi_J}) - \log(\frac{\pi_b}{\pi_J}) \newline
    = (\beta_{0a} + \beta_{1a}x) - (\beta_{0b} + \beta_{1b}x) \newline
    = (\beta_{0a} - \beta_{0b}) + (\beta_{1a} - \beta_{1b})x
$$

\subsubsection{Ordinal Categorical Responses}
\label{sec:orgaf59762}
When response categories are ordered, logits can utilize the ordering.
Using a cumulative logit function, the outcome for is the probability
that a value \(Y\) falls below a category \(J\). It looks like a binary
logistic regression model.

$$
    logit[Pr{Y \leq j}] \newline 
    = \log[\frac{Pr{Y \leq j}}{1 - Pr(Y \leq j)}] \newline 
    = \log[\frac{\sum_{i = 1}^j \pi_i}{\sum_{l = j + 1}^J\pi_l}] \newline
    = \beta_{0j} + \beta_1x
$$

\section{Log-Linear Models (Poisson Regression)}
\label{sec:org83338f7}
For Y, the number of successes in a given time or space interval. The
Poisson Distribution is most appropriate for counts of rare events that
occur at completely random points in space or time. Works reasonably
well for count data where spread increases with mean.

\subsection{Characteristics}
\label{sec:orgbb0f484}
\begin{itemize}
\item \(Var(Y) = \mu(Y) = \mu\)
\item Distribution tends to be right-skewed and is most pronounced when the
mean is small
\item Larger means tend to be well approximated by a normal distribution
\end{itemize}

Log link helps straighten the relationship between the predictors and
the response; however, variance will still be non-constant after the
transformation.

\subsection{Interpretation}
\label{sec:org54393dd}
Multiplicative effect on the mean. Can also convert to an estimated
percent increase. (\(e^{\beta_1} = proportion\)). This is different than
logistic regression where \(e^{\beta_1}\) gives the odds ratio.

\subsection{Model Assessment}
\label{sec:orgaa9e3bb}
\begin{itemize}
\item Scatterplots
\item Residuals

\begin{itemize}
\item Deviance Residual (more reliable for detecting outliers)
\item Pearson Residual
\end{itemize}

\item Deviance GoF
\end{itemize}

If Poisson means are at least 5 (large):
\begin{itemize}
\item Distribution of both residuals are approximately Standard Normal
\item If > 5\% of residuals exceed 2 in magnitude or if one or two greatly exceed 2, there are
\end{itemize}
problems in the fit

If Poisson means < 5 (small):
\begin{itemize}
\item Neither set of residuals follows a normal distribution well thus comparison to standard normal provides a
\end{itemize}
poor lack of fit

\textbf{Deviance GoF Test}: informal assessment of the adequacy of a fitted
model
\begin{itemize}
\item Use in conjunction with plots and tests of model terms
\item Large p-value indicates model is inadequate OR insufficient data to detect
\end{itemize}
inadequacies
\begin{itemize}
\item Small p-value indicates Model for the mean is incorrect OR Poisson is an inadequate model for the response OR a few severely
\end{itemize}
outlying observations contaminate the data

\subsection{Extra Poisson Variation (Over dispersion)}
\label{sec:org6828fd9}
Over disperson leads to higher variance in responses than predicted by
the Poisson Distribution.

\begin{itemize}
\item Unmeasured effects
\item Clustering of events
\item Other contaminating influences
\end{itemize}

\subsubsection{Checking for Over dispersion}
\label{sec:org1f9ba96}
\begin{itemize}
\item Is it likely?

\begin{itemize}
\item Are important explanatory variables not available?
\item Are individuals with the same level of explanatory variables
behaving differently?
\item Are events making up the count clustered or systematically space
rather than randomly spaced?
\end{itemize}

\item Fit a negative binomial model and check if \(\psi \gt 1\)
\item Compare Sample Variance to Sample Averages for groups of responses
with identical explanatory variable values
\item Examine Deviance GoF Test after fitting a rich model
\item Examine Residuals to see if a large deviance statistic may be due to
outliers
\end{itemize}

\section{Negative Binomial Regression}
\label{sec:org3502815}
Alternative to quasi-likelihood estimation in Poisson regression when
over dispersion is present is negative binomial regression. An
additional parameter \(\phi\) is used to model count variation.

\begin{itemize}
\item \(\mu{Y_i | X_{i1}, ..., X_{ip}} = \mu_i\)
\item \(Var(Y_i | X_{i1}, ..., X_{ip}) = \mu_i(1 + \phi\mu_i)\)
\end{itemize}

Strategies same as log-linear regression except no need to investigate
extra-poisson variation.

\section{Experiment Design}
\label{sec:org2994bf4}
\subsection{Studies}
\label{sec:org84ac288}
\subsubsection{Prospective}
\label{sec:org84988f0}
Subjects selected from or assigned to group with specified explanatory
factor levels then responses are determined. This is the traditional
experiment design.

\begin{enumerate}
\item Retrospective
\label{sec:org388c465}
Subjects selected from groups with specified response levels then their
explanatory factors are determined. Only the odds ratio (\(\phi\)) can be
estimated.

This is useful if response proportions are small which would normally
require huge samples in a prospective study (i.e. Cancer Rates). It is
also useful if there would be moral implications for conducting an
experiment in a prospective fashion (i.e. link between smoking and
cancer).

\item Matched Case-Control
\label{sec:org5d18f7e}
In a 2 x 2 table, case-control studies match a single control with each
case. For a binary response Y, each case (Y = 1) is matched with a
control (Y = 0) according to a certain criteria that could affect the
response. The study observes cases and controls on the predictor
variable X and analyzes the XY association.

Analysis uses \textbf{Conditional Likelihood Logistic Regression}. Each subject
has their own probability distribution. $$
    \log(\frac{\pi_{i1}}{1 - \pi_{i1}}) = \beta_{0i} + \beta_1
$$

$$
    \log(\frac{\pi_{i2}}{1 - \pi_{i2}}) = \beta_{0i}
$$

\(\beta_{0i}\) allows probabilities to vary among subjects. It can be
extended to K predictors but typically one variable is of special
interest while the others are controlled covariates.
\end{enumerate}

\subsection{Research Design Tool Kit}
\label{sec:org1912f44}
\textbf{Controls and Placebos}
\begin{itemize}
\item Control provides baseline
\item Placebo mimics new treatment in every aspect except the test ingredient
\end{itemize}

\textbf{Blinding}
\begin{itemize}
\item Subjects do not know which treatment is being received.
\end{itemize}
This eliminates the possibility that the end comparison measures
expectations rather than results

\textbf{Blocking}
\begin{itemize}
\item Arrange units into homogenous subgroups in which treatments
\end{itemize}
are randomly assigned to units in each block
\begin{itemize}
\item Strives to improve precision, control for confounding variables, and expand scope of
\end{itemize}
inference about treatment differences

\textbf{Stratification}
\begin{itemize}
\item Population units partitioned into homogenous subgroups (strata) and a random sample from each stratum is obtained
\end{itemize}

\textbf{Covariates}
\begin{itemize}
\item Auxilary measurements taken on each unit
\item Doesn't directly address the question but may be closely related
\item Controls for potentially confounding factors, improves precision, assess the
model, and expands scope of inference
\end{itemize}

\textbf{Randomization}
\begin{itemize}
\item Random Procedure to assign experimental units to different treatment groups
\item Controls for factors not explicitly controlled for in the design or the
analysis
\item Permits Causal inferences
\item provides a probability model for drawing inferences
\end{itemize}

\textbf{Random Sampling}
\begin{itemize}
\item Employ a Well-Understood random procedure to select units from a population
\end{itemize}

\textbf{Replication}
\begin{itemize}
\item Conducting copies of a basic study pattern
\item Refers to assigning one treatment to multiple units with each block
\item increased precision for treatment effects and improved model assessment
\end{itemize}

\textbf{Balance}
\begin{itemize}
\item Having the same number of units assigned to each treatment group
\item Optimize precision for treatment comparisons and ensure independence
\end{itemize}

\subsubsection{Improving Confidence Intervals}
\label{sec:org1fab148}
$$
    \bar{Y_1} - \bar{Y_2} \pm qt(1 - \alpha/2, n_1 + n_2 - 2)s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
$$

\begin{itemize}
\item qt reduced with replication or \(\alpha\)
\item \(s_p\) reduced by blocking, including covariates, or improving
measurement technology
\item square root term can be reduced by replication or balance
\end{itemize}

\subsubsection{Choosing a Sample Size}
\label{sec:orgf237809}
$$
    n = 4 \frac{[qt(1 - \alpha/2, n - k)]^2 s_\epsilon^2}{({Practically Significant Difference})^2} (\sum_{i = 1}^k C_i^2)
$$

where \(C_k\) is the kth coefficient in a linear combination of means \(g\).

\begin{enumerate}
\item Studies comparing 2 proportions
\label{sec:org5bef613}
\end{enumerate}

\subsubsection{Designing a Study}
\label{sec:org698801c}
\begin{enumerate}
\item State objective
\item Determine Scope of Interest

\begin{itemize}
\item Will it be randomized or observational?
\item What experimental or sampling units will be used?
\item What are the populations of interest?
\end{itemize}

\item Understand the system under study
\item Decide how to measure the response
\item List factors that can affect the response

\begin{itemize}
\item \textbf{Design factors}: factors to vary; factors to fix
\item \textbf{Confounding factors}: factors to control (blocking); factors to
control by analysis (covariates); factors to control by
randomization
\end{itemize}

\item Plan the conduct of the experiment (timeline)
\item Outline the statistical analysis
\item Determine Sample Size
\end{enumerate}

\subsubsection{Factorial Design}
\label{sec:orgffe9694}
\begin{enumerate}
\item 2$\backslash$\textsuperscript{2}
\label{sec:org3e655ec}

\item 2$\backslash$\textsuperscript{3}
\label{sec:org668dcac}

\item 2$\backslash$\textsuperscript{k}
\label{sec:org91208cd}
Coded variable form (-, +) useful for experimenter * Gives all effects
and interactions * T-statistics equivalent to F-statistics

Engineering Units form useful for others * Does not depend on
experimental levels or factors * Coefficients have a different
interpretation: a regression coefficient represent the effect of
changing a factor by 1 (engineering) unit, not the effect of changing
from low to high
\end{enumerate}
\end{document}
