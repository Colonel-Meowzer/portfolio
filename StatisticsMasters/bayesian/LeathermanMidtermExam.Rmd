---
title: "Take-Home Exam"
author: "Dustin Leatherman"
date: "November 3, 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 10)
library(rjags)
library(tidyverse)
library(knitr)
library(kableExtra)
library(grid)
library(gridExtra)
library(mice)

load("~/Downloads/exam.rdata")
```

# Background

The data used in this analysis are simulated measurements of the Normalized Difference Vegation Index (NVDI). This is commonly used indicate the "greeness" of a given area. Specifically, it is most often measured from satellites to monitor desertification. This fake dataset is generated from three fake satellites, each with their own pros and cons.

$Y_1: 365 \times 6$ matrix of unbiased, but noisy measurements $\theta_{tj}$ where $j = 1, ..., 6$ and $t = 1, ..., 365$. $Y_1$ is missing 80% of the time. The data can be considered Missing Completely At Random.

$Y_2$: A potentially biased (and noisy) measurement. This satellite only provides a measurement of $\theta_{1t}$.

$Y_3$: A potentially biased (and noisy) measurement. It estimates over the entire spatial domain of $\theta$. i.e. $\frac{1}{6} \sum_{j = 1}^{6} \theta_{tj}$

The data has been generated from the following models:

$$
\theta_1 \sim N(\mu_1, \Sigma_1), \ \theta_t | \theta_{t - 1} \sim N(\mu_2 + \rho \theta_{t - 1}, \Sigma_2)
$$
where $\mu_1, \mu_2$ are mean vectors, $\rho \in (0, 1)$ which controls temporal dependence, and $\Sigma_1, \Sigma_2$ are $6 \times 6$ covariance matrices.

# Statistical Model

Since the data were derived from a Normal distribution, it is appropriate to use a Normal distribution for it's likelihood functions.


## $Y_1: Y_1 \sim N(\theta_{tj}, \tau_{Y_1})$

The expected value of $Y_1$ should be the unbiased value in the Y1 dataset with some variance. The $Y_1$ likelihood is the most straightforward among the three likelihoods.

$\tau_{Y_1} \sim InvWishart(k = 7, R)$. K and R are hyper-parameters for the Wishart Distribution. They were chosen to be k = 7 and R a 5 x 5 a diagonal matrix with values of 0.1. The assumption for this prior is that a measurement for a given pixel is independent of other pixels. A small variance was chosen in order to make this uninformative.

$\theta_{t1}$ is a Normal Prior with mean $\mu_1 \sim N(0, 1000)$ and precision $\tau \sim InvGamma(0.1, 0.1)$. This is derived from the initial distribution of $\theta_{tj}$. Since there is correlation between the NVDI measurements for a given day and its surrounding days, that needs to be taken into account. Thus $\theta_{t, 2-6} \sim N(\mu_2 + \rho \theta_{t - 1,j})$ where $\rho \in (0,1)$. This can be represented with the uninformative Beta Prior, $\rho \sim Beta(1,1)$

## $Y_2: Y_2 \sim N(\theta_{t1} + \gamma_{Y_2}, \tau_{Y_2})$

$Y_2$ is a biased estimate of $\theta_1$ with some unknown variance. This is similar to $Y_1$ but with the addition of an  unknown bias $\gamma_{Y_2}$, which is represented as an uninformative Normal Prior. $\tau_{Y_2}$ is an uninformative Inverse Gamma Prior $\tau_{Y_2} \sim InvGamma(0.1, 0.1)$

## $Y_3: Y_3 \sim N(\gamma_{Y_3} + \frac{1}{6} \sum_{j = 1}^{6} \theta_{tj}, \tau_{Y_3})$

$Y_3$ is represented as an average across of all NVDI measurements plus some unknown bias $\gamma_{Y_3}$. Priors $\gamma_{Y_3}$ and $\tau_{Y_3}$ are modeled similarly to the other datasets.

\newpage

# JAGS

```{r}
unionJagsOutput <- function(jags_data) {
  data <- NULL
  for(chain in 1:length(jags_data)) {
    new.data <- 
      jags_data[[chain]] %>% 
        as_tibble() %>% 
        mutate(chain = factor(chain), row_num = row_number()) %>%
        pivot_longer(starts_with("theta"), names_to = "col", values_to = "value")
    
    if(is.null(data)) {
      data <- new.data
    } else {
      data <- union_all(data, new.data)
    }
  }
  return (data)
}

```


```{r}
n <- max(nrow(Y1),length(Y2), length(Y3))
p <- 6

# set covariance matrix for Inverse Wishart Prior. 
# Assumes high variance with no covariance between Pixels.
R <- diag(rep(0.1, p - 1))

data <- list(Y1 = Y1, Y2 = Y2, Y3 = Y3, n = n, R = R)
params <- c("theta")

# Settings (automatically calculates the number of iterations needed based on inputs)

nBurn <- 20000
nChains <- 3
nSave <- 6000
nThin <- 10
nIter <- ceiling((nSave*nThin)/nChains)
```

```{r, fig.width=10, fig.height=15, echo=TRUE}
model_string <- textConnection("model{
  # Likelihood
  for(i in 2:n){
    for(j in 1:6){
      Y1[i, j] ~ dnorm(theta[i, j], tauY1)
    }
    Y2[i] ~ dnorm(theta[i, 1] + gammaTheta2, tauY2)
    Y3[i] ~ dnorm(((theta[i, 1] + theta[i, 2] + theta[i, 3] + theta[i, 4] + theta[i, 5] + theta[i, 6])/6), tauY3)
    theta[i, 1] ~ dnorm(mu[1], tauTheta1)
    theta[i, 2:6] ~ dmnorm(mu[2:6] + rho * theta[i - 1, 2:6], sigmaTheta2)
  }
  

  # Set first row separately since there is serial-correlation in the model.  
  for(j in 1:6){
    Y1[1, j] ~ dnorm(theta[1, j], tauY1)
  }
  Y2[1] ~ dnorm(theta[1, 1] + gammaTheta2, tauY2)
  Y3[1] ~ dnorm(((theta[1, 1] + theta[1, 2] + theta[1, 3] + theta[1, 4] + theta[1, 5] + theta[1, 6])/6) + gammaTheta3, tauY3)

  theta[1, 1] ~ dnorm(mu[1], tauTheta1)
  theta[1, 2:6] ~ dmnorm(mu[2:6], sigmaTheta2)

  # Priors
  for(z in 1:6) {
    mu[z] ~ dnorm(0, 0.001)
  }
  tauY3 ~ dgamma(0.1, 0.1)
  tauY2 ~ dgamma(0.1, 0.1)
  tauY1 ~ dgamma(0.1, 0.1)
  tauTheta1 ~ dgamma(0.1, 0.1)
  gammaTheta2 ~ dnorm(0,0.001)
  gammaTheta3 ~ dnorm(0,0.001)
  rho ~ dbeta(1,1)
  # k = 7 makes the expected value of sigma equal to R
  sigmaTheta2 ~ dwish(R[,], 7)
}")

model <- jags.model(model_string,data=data,n.chains=nChains, quiet = TRUE)
update(model,burn=nBurn,progress.bar="none")
model1.out <- coda.samples(model,variable.names=params,thin=nThin,n.iter=nIter, progress.bar = "none")
unioned.data <- unionJagsOutput(model1.out)
```

\newpage

# Convergence Diagnostics

```{r, fig.width=10, fig.height=4}
unioned.data %>% 
  filter(str_detect(col, "theta\\[(1|100|365),\\d+\\]")) %>% 
    ggplot(aes(x = row_num, y = value, color = chain)) + 
      geom_line() + 
      facet_wrap(~col, ncol = 6, scales = "free") +
      labs(x = "Iteration", y = "Posterior Value", caption = "Simulation of Days 1, 10, 100, and 365.", title = "Sample of Trace Plots for Theta")

```

Pictured are Trace plots for a sample of days between 1 and 365. A sample of days are displayed due to the sheer number of parameters. The three chains appear to overlap decently well for the variables of interest which suggest that the parameters for the models have converged.


```{r, fig.width=10, fig.height=3}
g.out <- geweke.diag(model1.out)

geweke.data <- NULL
for(chain in 1:nChains){
  new.data <- g.out[[chain]]$z %>% 
    as.data.frame() %>% 
    rownames_to_column(var = "col") %>% 
    rename(value = ".") %>% 
    mutate(
      chain = factor(chain)
    )
  
  #colnames(new.data) <- c("col", "chain")
  if(is.null(geweke.data)) {
    geweke.data <- new.data  
  } else {
    geweke.data <- union_all(geweke.data, new.data)
  }
}

geweke.data <- 
  geweke.data %>%
    mutate(
      col = str_replace_all(col, "(\\[|,)", "_") %>% str_remove("\\]"),
      sim_row = str_split(col, "_", simplify = TRUE)[,2],
      group = str_split(col, "_", simplify = TRUE)[,3]
    )

geweke.data %>% 
  ggplot(aes(x = value, fill = chain)) + 
    geom_histogram() + 
    facet_wrap(~group) +
    geom_vline(xintercept = -2, linetype = "dashed") +
    geom_vline(xintercept = 2, linetype = "dashed") +
    scale_color_discrete(name = "Chain") +
    labs(x = "Geweke Statistic", y = "Density", title = "Geweke Distribution per Pixel", caption = "Histograms highlighting the convergence of the chains using the Geweke Statistic. It is ideal to be between the dotted lines.")
```

Most of the Geweke Statistics fall under abs(2) though all thetas have some values that fall outside. This indicates that some runs of the parameters have not converged.

```{r}
gelman.out <- gelman.diag(model1.out)
gelman.out$psrf %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "col") %>% 
  as_tibble() %>% 
  summarise_at(c("Point est.", "Upper C.I."), list(quantile)) %>% 
  mutate(
    quantile = c("min", "0.25", "0.5", "0.75", "max")
  ) %>% 
  select(quantile, "Point est.", "Upper C.I.") %>% 
      kable(
        caption = "Gelman-Rubin Statistic Quantiles to measure convergence of chains"
      ) %>% 
      kable_styling(full_width = T, bootstrap_options = "striped", latex_options = "hold_position")

```

The Gelman Statistics are close to 1 indicating that convergence has been reached.

Despite the magnitude in some of the Geweke statistics, it appears that convergence for this model has been attained based on the outcome of the Trace Plots, Gelman-Rubin Statistics, and Geweke Statistics..

\newpage

# Final Results

```{r, fig.width=10, fig.height=8}
Y1.data <- 
  as.data.frame(Y1) %>% 
    mutate(day = row_number()) %>% 
    as_tibble() %>% 
    pivot_longer(starts_with("Y1"), names_to = "col", values_to = "value") %>% 
    mutate(
      pixel = str_sub(col, start = -1L)
    ) %>% 
    select(day, pixel, value)

Y2.data <-
  data.frame(value = Y2) %>% 
  as_tibble() %>% 
  mutate(day = row_number(), pixel = "1") %>% 
  select(day, pixel, value)

Y3.data <-
  data.frame(value = Y3) %>% 
  as_tibble() %>% 
  mutate(day = row_number()) %>% 
  select(day, value)

post.means <-
  unioned.data %>% 
    group_by(col) %>% 
    summarise(value=mean(value)) %>% 
    mutate(
      col = str_replace_all(col, "(\\[|,)", "_") %>% str_remove("\\]"),
      day = as.integer(str_split(col, "_", simplify = TRUE)[,2]),
      pixel = str_split(col, "_", simplify = TRUE)[,3]
    ) %>% 
  left_join(Y1.data, by = c("day", "pixel"), suffix = c(".post", ".y1")) %>% 
  left_join(Y2.data, by = c("day", "pixel"), suffix = c(".post", ".y2")) %>% 
  left_join(Y3.data, by = c("day"), suffix = c(".post", ".y3")) %>% 
  rename(value.y2 = value.post.post)


post.means %>% 
  ggplot(aes(x = day)) + 
  geom_line(aes(y = value.post, color = "Posterior Mean")) + 
  geom_point(aes(y = value.y1, color = "Y1 Actual"), alpha = 0.6) + 
  geom_point(aes(y = value.y2,  color = "Y2 Actual"), alpha = 0.6) + 
  geom_point(aes(y = value.y3, color = "Y3 Actual"), alpha = 0.6) + 
  facet_wrap(~pixel, scales = "free") +
  labs(x = "Day", y = "NVDI", title = "NVDI Prediction per Pixel") + 
  scale_color_manual("", 
                     breaks = c("Posterior Mean", "Y1 Actual", "Y2 Actual", "Y3 Actual"), 
                     values = c("black", "red", "blue", "purple"))
```

The Posterior mean accurately tracks the golden-image data (red) when available for all pixels. The Y3 Actuals (average-valued measurements) compares well to the posterior mean for Pixel 1, 2, and 5 but not so well for the others. The posterior mean does not follow the Y2 Actuals for pixel 1. Rather it seems closer to Pixel 2 or 4 in terms of shape. This model seems to accurately track the golden-image datapoints fairly closely and is bolstered in certain cases by the other datasets. Overall, I would consider this model to be fairly decent in predicting NVDI.

