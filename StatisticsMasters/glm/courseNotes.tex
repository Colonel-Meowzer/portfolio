% Created 2020-04-17 Fri 16:38
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Dustin Leatherman}
\date{\today}
\title{Generalized Linear Models Notes}
\hypersetup{
 pdfauthor={Dustin Leatherman},
 pdftitle={Generalized Linear Models Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Types of Regression}
\label{sec:orga262ebd}
\subsection{Logistic}
\label{sec:org82cf2e7}
A measure of the relationship between categorical data using a Binary
distribution. (Bernoulli for Logistic, Gaussian for OLS Regression).

\(E(y) = \frac{1}{1 + e^{-(a + \beta_k x_k)}}\)

\begin{itemize}
\item Deviance is used to measure lack-of-fit. (instead of Sum of Squares)
\item Likelihood Ratio Test (LRT) also used
\end{itemize}

\subsubsection{Examples}
\label{sec:org2cd6c90}
\begin{itemize}
\item Predicting whether a political candidate wins an election
\item Predicting Admission into a Program
\end{itemize}
\subsection{Poisson}
\label{sec:org0733c8d}
Used for Count data. Y refers to the \textbf{number of occurrences} of an event and is
assumed to have a Poisson Distribution:

\(P(Y = k | x_1, x_2, ..., x_m) = \frac{e^{-\mu} \mu^k}{k!}\)

\textbf{Log Link}: \(y = exp(\beta_0 + \Sigma \beta_i x_i)\)

The model is used for goodness-of-fit tests. Recall the following about the
Poisson Distribution:

\begin{equation}
\begin{split}
var(Y) = & \mu\\
E(Y) = & \mu
\end{split}
\end{equation}

\subsubsection{Mean > Variance}
\label{sec:org1accf42}
\begin{itemize}
\item Use Logistic Regression to adjust standard errors
\item Use Negative Binomial Regression
\end{itemize}

\subsubsection{Over dispersion}
\label{sec:orge281343}
This occurs when the observed variance is larger than the assumed (theoretical) variance.

\subsubsection{Interpretation}
\label{sec:orgf452836}
Exponentiated coefficients are multiplicative analgous to odds ratios but called
\emph{incidence rate ratios}.

For example, if \(e^b = 2\), then rate doubles for each unit change in x. If \(e^b
= 0.5\), then rate halves. If \(e^b = 5\) and x decreases, then \(\frac{1}{5} = 0.2\)

\subsubsection{Examples}
\label{sec:org7a4fd6a}
\begin{itemize}
\item Number of people in line in front of you at the grocery store
\item Number of awards earned by students at one High school
\end{itemize}

\subsection{Negative Binomial}
\label{sec:orga9ad612}
Used for Count Data. Describes probabilities of the occurrence of whole numbers greater than or equal
to zero. Y is the number of times an event has occurred. One parameterization
is:

\(P(y) = P(Y | y) = \frac{\Gamma (y + \frac{1}{a})}{\Gamma (y + 1) \Gamma
(\frac{1}{a})} (\frac{1}{1 + a \mu})^{\frac{1}{a}}\), \(\mu > 0, a > 0\)

\(a\) is the heterogeneity parameter.

The traditional model is: \(log(\mu) = \beta_0 + \Sigma \beta_i x_i\)

The response variable may be over or under-dispersed.
The model models the log of expected count as a function of predictors.

\subsubsection{Model Fit Statistics}
\label{sec:orgba142e2}
\begin{itemize}
\item Log-likelihood
\item deviance
\item Pearson chi-square dispersion
\item AIC
\item BIC
\end{itemize}

\subsubsection{Interpretation}
\label{sec:orga316ea4}
For one unit of change in the predictor var, the difference in the logs of
expected counts in the response is expected to change by \(\beta_i\), given that
all other predictors are held constant.

\subsubsection{Examples}
\label{sec:orgd8243fc}
\begin{itemize}
\item Attendance behavior based on enrolled program and a standardized test score
\item Number of hospital visits by seniors based on characteristics and types of
health plans.
\end{itemize}
\subsection{Gamma}
\label{sec:org607952a}
Used for continuous, positive, right-skewed data where the variance is nearly
constant.

\textbf{Log Link}: \(\mu = exp(\beta_0 + \Sigma \beta_i x_i)\)
\subsubsection{Examples}
\label{sec:org9e30212}
\begin{itemize}
\item Study of damage done to cars in an insurance claim/
\end{itemize}

\subsection{Zero-Inflated Poisson (ZIP)}
\label{sec:org32cec25}
Used to model count data with an excessive amount of zeros. There are two parts
to this model.

Both Poisson Count and Logit Zero models should have good predictors. They are
not required to have the \emph{same} predictors.

This should be used for large sample sizes.

\subsubsection{Poisson Count model}
\label{sec:org9a3ed04}
Generates counts, some of which may be zero.

\(P(y_j = h_i) = (1 - \pi) \frac{\lambda^{h_i} e^{- \lambda}}{h_i !}\), \(h_i \geq 1\)

\begin{itemize}
\item \(y_j\): any non-negative integer value
\item \(\lambda_i\): expected Poisson count for the ith individual
\item \(\pi\): probability of extra zeros
\end{itemize}

\(E(Y) = (1 - \pi) \lambda\)
\(var(Y) = \lambda (1 - \pi) (1 - \lambda \pi)\)

\subsubsection{Logit ``Zero'' Model}
\label{sec:org7d17b4b}
Used for predicting excess zeros. This is a binary distribution that generates zeros.

\(P(y_i = 0) = \pi + (- \pi) e^{- \lambda}\)

Issues that can occur
\begin{itemize}
\item Perfect Prediction
\item Separation or Partial Separation
\end{itemize}

\subsection{Zero-Inflated Negative Binomial}
\label{sec:org79855b5}
Similar to ZIP. This is used for over-dispersed count response variables. The
Count model in this case is \textbf{Negative Binomial} instead of Poisson.

\section{Exponential Family of Distributions}
\label{sec:org2b73379}
A distribution belongs to the exponential family if it can be written in the
following form:
\begin{equation}
\begin{split}
f(y : \theta) = & s(y) t(\theta) e^{a(y) b(\theta)}\\
= & exp[a(y) b(\theta) + c(\theta) + d(y)]
\end{split}
\end{equation}

\begin{center}
\begin{tabular}{llll}
Distribution & Natural Parameter & c & d\\
\hline
Poisson & log \(\theta\) & \(- \theta\) & \(- log y!\)\\
Normal & \(\frac{\mu}{\sigma^2}\) & \(\frac{- \mu^2}{2 \sigma^2} - 0.5 log (2 \pi \sigma^2)\) & \(- \frac{y^2}{2 \sigma^2}\)\\
Binomial & \(log(\frac{\pi}{1 - \pi})\) & \(n log (1 - \pi)\) & \(log (n choose y)\)\\
\end{tabular}
\end{center}


\subsection{Properties}
\label{sec:orgf34979b}
\begin{equation}
\begin{split}
E(a(Y)) = & - c'(\theta) / b' (\theta)\\
var(a(Y)) = & \frac{b'' (\theta) c' (\theta) - c''(\theta) b' (\theta)}{[b' (\theta)]^3}
\end{split}
\end{equation}

\subsubsection{Score Statistic}
\label{sec:orga28ea07}
\begin{equation}
\begin{split}
l(\theta; y) = & a(y) b(\theta) + c(\theta) + d(y)\\
U(\theta; y) = & \frac{dl(\theta; y)}{d \theta} = a(y) b' (\theta) + c' (\theta)\\
E(U) = & b' (\theta) E[a(Y)] + c' (\theta)\\
= & b' (\theta) \frac{- c' (\theta)}{b' (\theta)} + c' (\theta) = 0\\
var(U) = & [b' (\theta)^2] var[ a(Y)]\\
= & b'' \frac{(\theta) c' (\theta)}{b' (\theta)} - c'' (\theta)\\
var(U) = & E(U^2) = - E(U')
\end{split}
\end{equation}

\begin{itemize}
\item U: A random variable called the \textbf{Score Statistic}
\item var(U): Information Matrix
\end{itemize}
\section{Outliers \& Influential Obs}
\label{sec:org1082f63}
\subsection{Explanatory Variable Pattern (EVP)}
\label{sec:org703ef13}
Sometimes, converting bernoulli random variables to binomial is helpful for
running goodness-of-fit measures and residuals.

This format has one row for each unique set of explanatory variables. Suppose
there are 6 observations with a bernoulli RV value of 1 and an Age of 30. This
would be converted to a single row with (age=30, n=6, fail=5, y=1)

When fitting models in this form, use
\begin{R}
mod.fit <- glm(y/n \textasciitilde{} B1, data = \ldots{})
\end{R}
\subsection{Pearson Residual}
\label{sec:orgd08bfa8}
Pearson Residual: \(e_j = \frac{\text{observed -
predicted}}{\sqrt{\hat{Var}(Observed)}} = \frac{y_j - n_j \hat \pi_j}{\sqrt{n_j
\hat \pi_j (1 - \hat \pi_j)}}\)
with a binomial version of the data, there's a possibility the sample size is
large enough for normal approximation to work. With continuous variables, this
is not the case and this residual should be interpreted with caution.

Outliers: \(\pm 2.576\) though the effect on the model should be examined.

Standardized Pearson Residual: \(e_j = \frac{e_j}{\sqrt{1 - h_j}} = \frac{y_j -
n_j \hat \pi_j}{\sqrt{n_j \hat \pi_j (1 - \hat \pi_j)(1 - h_j)}}\)

where \(h_j\) is the jth diagonal of the hat matrix.
\subsection{Pearson Statistic}
\label{sec:orgd679246}

\(\chi^2 = \sum_{j = 1}^{J} e_j^2\)
J: num of explanatory variable patterns.

Can be approximated by \(\chi_{J - (k + 1)}^2\) distribution where k + 1 is the
num of parameters estimating.

\begin{equation}
\begin{split}
H_0: & logit(\pi) \alpha + \beta_1 x_1 + ... + \beta_k x_k \ \text{k + 1 parameters}\\
H_A: & \text{Saturated Model (J parameters)}
\end{split}
\end{equation}

The ``saturated'' model contains an estimate per explanatory variable pattern.

\subsubsection{Influential Obs}
\label{sec:orgab713aa}

\(\chi^2 \approx e_j^2\) (squared standardized residual) is used to calculate the
influence. The same statistic is used for outliers.

\(e_j^2 > \chi_{0.95,1}^2 = 3.84\) or \(e_j^2 > \chi_{0.99,1}^2 = 6.63\) may
indicate an outlier or influential EVP.

A measure similar to Cook's distance can also be used.

\(\Delta \hat \beta_j = \frac{e_j^2 h_j}{(1 - h_j)}\)

Large values indicate an explanatory variable pattern may be influential. Its
effect on the \(\hat \beta\)'s can be seen by temporarily removing the variable
from the dataset and refitting the model.

\subsection{Diagnostic Plots}
\label{sec:orgf7377ea}
\begin{itemize}
\item When there is one explanatory var, plotting \(e_j\), \(e_j^2\) and/or \(\Delta \hat
  \beta_j\) is helpful. Doesnt work if the explanatory variable is binary.
\item \(e_j\), \(e_j^2\) and/or \(\Delta \hat \beta_j\) vs the observation number
\item \(e_j^2\) and \(\Delta \hat \beta\) vs the estimated probabilities \textbf{or} proportion
to \(n_j\).
\item \(e_j^2\) vs estimated probabilities with the plotting point proportional to
\(\Delta \hat \beta\) can help combine different influence measures.
\end{itemize}
\subsection{Model Selection Process}
\label{sec:orgcdd71af}

What explanatory variables should be in the model? Should interactions or
quadratic terms be included?

\begin{enumerate}
\item Find all possible one variable logistic regression models.
LRT is preferred to test model parameters with Logistic Regression due to the
\(\chi^2\) approximation for the LRT statistic.
\item Put all variables found in 1 in a logistic regression model. Perform
backwards elimination.
\item Determine if the quadratic or interaction terms are needed in the model. The
best way is to add them and see if they are significant.
\item Convert data to explanatory variable pattern
\item Examine how well the model fits the data. Make any changes necessary.

Calculate Pearson residuals, standardized residuals, Pearson Statistic,
\(\Delta \hat \beta\)'s and LRT. Construct diagnostic plots and determine if
changes need to be made.
\item Use the model.
\end{enumerate}
\section{Linear Mixed Models (LMM)}
\label{sec:orgb3de8a0}
Mixed Modeling is a model with both \emph{random} and \emph{fixed} effects.

\begin{itemize}
\item It is often found in Physical, Biological, Medicine, and Social Sciences.
\item Can be used in Longitudinal Studies
\item Can be used for clusters where there is staggered entry, dropout, missing
data, and mistime visits.
\end{itemize}

\textbf{Example Study}

Suppose a study involving different treatments (A, B, Control) to groups of rats
were conducted.
\subsection{Fixed Effect}
\label{sec:org0658e7f}
An effect which is generally being tested for. Characteristics such as Gender, Age, and Blood Type would be
considered fixed effects if they are used as independent variables.

\(Y_i = \beta_0 + \beta_1 Age + \beta_2 Gender + \beta_3 BloodType + \epsilon_i\)

Helps explain the variance of Y at each level of the data.

\subsection{Random Effect}
\label{sec:org3c9652d}
An effect which is being considered a sample from a much larger distribution.
Values for these are treated as \emph{random samples}.

They are variables specific to the data sample.
\begin{itemize}
\item Allow us to account for correlation among observations within the same level-2
or higher units.
\begin{itemize}
\item correlations among observations within the same school
\end{itemize}
\item Allow us to partition the total variance of Y into levels that correspond with
the multilevel structure of the data.
\begin{itemize}
\item How much of the variation in student math scores can be attributed to
student-level variability (level 1) vs school-level variability (level 2)?
\end{itemize}
\end{itemize}

\(Y_i = \beta_0 + \beta_1 Age + \beta_2 Gender + \beta_3 BloodType + b_{0j} + \epsilon_{ij}\)

\(b_{0j}\): Cluster-specific random deviations
\(\epsilon_{ij}\): Subject-within-cluster-specific errors

This LMM is referred to as the \textbf{Variance Components} model because it partitions
the total variation in the outcome into between-cluster variation and
within-cluster variation.
\begin{itemize}
\item variance of the random intercepts is the between-cluster variation. Also
referred to as the Level 2 variance.
\item Variance of the residuals is the within-cluster variation, known as the Level
1 variance.
\end{itemize}
Introduction of a \emph{Random Effect} creates a variance component.

\(Var(ij) = \sigma_p^2 + \sigma_r^2\)

Random effects usually include a random intercept for each level of clustering
to account for possible correlation within clusters, and to make inference to
the larger population of clusters.
\subsection{Multi-Level Data}
\label{sec:orgf0c5338}

Level 1 is the smallest grain of data where the outcome variable of interest is
measured.

Levels 2+ capture higher level information.
\begin{itemize}
\item cluster-levels for Clustered Data
\item Subject-level for Longitudinal Data
\item Subject and cluster levels for clustered-longitudinal data
\end{itemize}

\textbf{Questions to Drive Analysis}
\begin{itemize}
\item Is the data ``clustered'', ``longitudinal'', or ``clustered-longitudinal''?
\item How many levels are there? 2, 3, or more?
\item What defines each level?
\item What is the outcome of interest and is it measured at Level 1?
\item What other variables are of interest at each level?

Using single-level (OLS, GLM) analysis leads to:
\begin{itemize}
\item Unit of Analysis problem
\begin{itemize}
\item School or child?
\end{itemize}
\item Aggregation Bias
\begin{itemize}
\item School SES or child SES?
\end{itemize}
\item Incorrectly estimated precision or standard errors
\begin{itemize}
\item incorrect p-values and thus conclusions
\end{itemize}
\end{itemize}
\end{itemize}

\subsubsection{Clustered Data}
\label{sec:org1c039cd}
An outcome is measured once for each subject, and subjects ``belong to'' clusters,
such as families, schools, or neighborhoods. These outcomes are likely to be
correlated with other members of the ``cluster''.

Each ``Level'' represents a factor that can be thought of as a random sample from
a larger population.
\begin{itemize}
\item students in a two-level clustered dataset can be thought of as a random sample
of students within each school.
\end{itemize}

\subsubsection{Repeated Measures}
\label{sec:orgbc167d7}
Multiple observations for one treatment level \textbf{or} the same subject.

\subsubsection{Examples}
\label{sec:orgcf6bfb0}

\begin{center}
\begin{tabular}{lll}
Level 3 & Level 2 & Level 1\\
\hline
 & School & [Student 1, Student 2, Student 3]\\
 & Child & [MeasurementTime 1, MeasurementTime 2]\\
 & Rat & [Treatment A (Region 1), Treatment A (Region 2)]\\
School & Classroom & [Student 1, Student 2, \ldots{}]\\
School & Student & [Score Grade 1, Score Grade 2, \ldots{}]\\
\end{tabular}
\end{center}
\end{document}
