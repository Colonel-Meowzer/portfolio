% Created 2020-05-16 Sat 14:53
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\author{Dustin Leatherman}
\date{\today}
\title{Think Globally, Fit Locally: Summary}
\hypersetup{
 pdfauthor={Dustin Leatherman},
 pdftitle={Think Globally, Fit Locally: Summary},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

This document consists of a Summary of ``Think Globally, Fit Locally: Unsupervised
Learning of Low Dimensional Manifolds'' by Lawrence K. Saul and Sam T. Rowels.

The three-pass technique is utilized here.

\section{Three-Pass Technique}
\label{sec:org200fa0b}

For the uninitiated, the three-pass technique is a method for reading dense
article or papers that breaks down reading into three steps.
\begin{enumerate}
\item Read the Abstract and headers.
\item Read the First sentence of each paragraph from end-to-end.
\item Read the entire paper end-to-end.
\end{enumerate}

For (1), each header in this document pertains to a header of the paper.
For (2), the first sentence of each paragraph has been paraphrased. If the first
sentence was not descriptive enough, the second sentence was used. Any formulas
from a previous paragraph that was referenced in the first sentence of the
following paragraph, the formula was recorded.
For (3), a ``Third Pass'' sub-header following the paragraph contains details
picked up on the third read.

The conclusion of the Three-pass technique should result in a reconstruction of the
original paper.

\section{Notations}
\label{sec:org2396412}

There are not any mathematical notations that deviate from the norm in this
document.

\begin{quote}
Comments or questions interjected by myself are placed in ``quote'' bodies to
provide delineation between the source material and my own thoughts as I grapple
its meaning.
\end{quote}

\section{Keywords}
\label{sec:orgb26bf41}
\begin{itemize}
\item Dimension Reduction
\item Manifolds
\item Locally Linear Embedding
\item Unsupervised Learning
\end{itemize}

\section{Introduction}
\label{sec:org8fafb44}

\begin{quote}
The introduction contains a series of definitions of key concepts gleaned from
the paper. This is different than other sections in that extra care and
attention was spent upfront in ensuring the definitions of key terms were understood.
\end{quote}

\textbf{Preprocessing}: Obtain more useful representations of raw signals for
 subsequent operations. i.e. classification, denoising, interpolation,
 visualization, outlier detection.

\textbf{Unsupervised Learning}: Framework of automatic methods to discover hidden structures of
 statistical regularities.

\textbf{Density Estimation}: Learn parameters of a prob. model for prediction (like \(\lambda\) in
Poisson)

\textbf{Dimensionality Reduction}: Compact representations of original data which
 contain enough information to make decisions.

\begin{quote}
Dimensionality Reduction is applied here in a non-prob and non-parametric setting.
\end{quote}

\textbf{Problem trying to be addressed}: Compute a low dim embedding of high dim data
 sampled with noise from an underlying manifold. Intersection of Geometry +
 Statistics. Use this to discover the \emph{few degrees of freedom} underlying the
 observations.

\begin{quote}
\textbf{Manifold}: A topological space that locally represents a Euclidean space near
 each point. Manifolds are continuous (homeomorphic) to n-dimensions. Manifolds cannot intersect (like a figure 8). Examples include: a
 line, a circle, Surfaces (plane, sphere, torus). A Manifold may \emph{not} be
 homeomorphic beyond n-dimensions.
\end{quote}

\textbf{PCA}: Compute linear projections of greatest variance from top eigenvectors
 of the covariance matrix.

\textbf{Multidimensional Scaling (MDS)}: Compute low dim embedding that best preserves pairwise distances between points.

If using euclidean distance, MDS equivalent to PCA. Both powerful \emph{Linear}
methods for dim reduction

\textbf{Locally Linear Embedding (LLE)}: Dimension Reduction technique for non-linear
 data. Involves a sparse eigenvalue problem that scales well to high dim
 datasets.
\begin{itemize}
\item More accurate for reconstructing manifolds in lower dimensions than
\end{itemize}
linear methods (PCA/MDS)

\section{Algorithm}
\label{sec:org1587b14}
\subsection{1}
\label{sec:orgc36deeb}
Based on simple geometric intuitions. Computes low dim embedding with the
property that nearby points in the high dim space are nearby and co-located with
respect to one another in the low dim space.

\subsubsection{Third Pass}
\label{sec:org9ba6a15}
\begin{itemize}
\item Embedding optimized to preserve local configurations of nearest neighbors.
\begin{quote}
This is accomplished by calculating weights based on the distance between
K-nearest neighbors and using those weights in the Embedding cost function.
\end{quote}

\item LLE doesn't need to use measures of distance or relation to far away points
\begin{quote}
This is because the algorithm restricts distance measures to the K nearest neighbors.
\end{quote}
\end{itemize}
\subsection{2}
\label{sec:orgfca3f36}
Data consists of N real-valued vectors \(\vec X_i\) of dimensionality D, sampled
from an underlying smooth manifold.

\subsubsection{Third Pass}
\label{sec:orgba123c7}

\begin{quote}
What does ``Smooth'' mean in mathematical terms?
\end{quote}

\begin{itemize}
\item Each data point and its neighbors should lie on or close to the manifold.

\begin{quote}
In this case, the manifold is equivalent to the original image so when talking
about being close to the manifold, this means that sampled points shouldn't be
far-fetched from the original points.
\end{quote}

\item ``Smooth'' and ``well-sampled'' mean that the data point has on the order of 2d
neighbors which define an approximately linear patch on the manifold.

\begin{quote}
This allows us to treat \(\vec X_i\) as a linear combination of its neighbors.
Later, this allows an approximation of \(\vec X_i\) to be constructed based off
its neighbors.

\begin{itemize}
\item How many neighboring data points are considered orthogonal?
\end{itemize}
\end{quote}
\end{itemize}
\subsection{3}
\label{sec:org6115d8c}
In the simplest form of LLE, identify the K nearest neighbors per data point by
Euclidean Distance.

\textbf{Reconstruction Errors - Cost Function}

\begin{equation}
\begin{split}
E(W) = \Sigma_i |\vec X_i - \Sigma_j W_{ij} \vec X_j|^2 \label{eq:1}
\end{split}
\end{equation}

\subsubsection{Third Pass}
\label{sec:orgca3d201}
\begin{itemize}
\item Reconstruction Cost Function adds up squared distances between all points and
their reconstructions.
\end{itemize}

\(W_{ij}\): contribution of the jth data point to the ith reconstruction.

Weights are computed using Least Squares with two constraints
\begin{enumerate}
\item Sparseness
\begin{itemize}
\item Each \(\vec X_i\) is reconstructed from only its neighbors.
\item \(W_{ij} = 0\) if \(\vec X_j\) not in the set.
\end{itemize}
\item In-variance
\begin{itemize}
\item \(\Sigma_j W_{ij} = 1\)
\end{itemize}
\end{enumerate}

\subsection{4}
\label{sec:org5b82e5a}
The constrained weights that minimize the reconstruction errors have several
important symmetries: For any data point, they are invariant to rotations,
rescalings, and translations from that data point to its neighbors. This also
means they are invariant to global rotations, rescalings, and translations.


\textbf{Embedding Cost Function}

\begin{equation}
\begin{split}
\Phi(Y) = \Sigma_i |\vec Y_i - \Sigma_j W_{ij} \vec Y_j|^2 \label{eq:2}
\end{split}
\end{equation}

\subsubsection{Third Pass}
\label{sec:org56b04d0}
\begin{itemize}
\item Invariance from Rotations and rescaling follows from \(\eqref{eq:1}\)

\item Invariance to translation enforced by sum-to-one constraint.
\item Reconstruction of weights \textbf{not} invariant to shear transformations.

\begin{quote}
Shear mapping is a linear map that displaces each point in a fixed direction,
by an amount proportional to its signed distance from the line that is
parallel to that direction and goes through the origin.

\textbf{Horizontal Shear example}

  \begin{bmatrix}
x'\\ y'
\end{bmatrix} = \begin{bmatrix}
x + my\\ y
\end{bmatrix} = \begin{bmatrix}
1 & m\\ 0 & 1
\end{bmatrix} \begin{bmatrix}
x\\ y
\end{bmatrix}

\textbf{Vertical Shear Example}

   \begin{bmatrix}
x'\\ y'
\end{bmatrix} = \begin{bmatrix}
x\\ mx + y
\end{bmatrix} = \begin{bmatrix}
1 & 0\\ m & 1
\end{bmatrix} \begin{bmatrix}
x\\ y
\end{bmatrix}

I believe its not invariant because a translation or rescaling affects all
elements of a matrix whereas a shear transformation affects individual elements.
This means that the matrix doesn't keep the same ``shape''?
\end{quote}
\end{itemize}

This means that the reconstruction weights do not depend on a particular frame
of reference.
\subsection{5}
\label{sec:org3638f82}
\textbf{Steps}
\begin{enumerate}
\item Compute neighbors of each \(\vec X_i\)
\item Compute weights \(W_{ij}\) that best reconstruct each \(\vec X_{i}\) from its
neighbors, minimizing the Weight cost function \(\eqref{eq:1}\) by using constrained Linear fits.
\item Compute \(\vec Y_i\) best reconstructed by \(W_{ij}\), minimizing the quadratic
form \(\eqref{eq:2}\) by its bottom non-zero eigenvectors.
\end{enumerate}

\subsection{6}
\label{sec:orgf7e576f}
Suppose data lie on or near a manifold of \(d << D\). To a good approximation, we
imagine there exists a linear mapping that maps the high dim coords to each
neighborhood to global internal coords on the manifold.
\subsection{7}
\label{sec:orgfeebf52}
Imagine cutting out locally linear patches of the manifold and rearranging them
in the low dim embedding space. If the placement of each patch involves no more
than a translation, rotation, and/or rescaling, then angles between data points
will be preserved.
\subsection{8}
\label{sec:org3299395}
LLE constructs a neighborhood mapping on the above idea.

\subsection{9}
\label{sec:org60514f1}
The embedding cost function \(\eqref{eq:2}\) defines a quadratic form in the
outputs \(\vec Y_i\). Subject to constraints that make the problem well-posed, the
cost function has a unique global minimum.
\subsection{10}
\label{sec:org6745020}
Note that while reconstruction weights for each data point are computed from its
local neighborhood (independent of the weights for other data points) the
embedding coordinates are computed by an \(N \times N\) eigensolver, a global
operation that couples all data points that lie in the same connected component
of the graph defined by the neighbors.

\begin{quote}
Is ``connected'' the same as continuous and homeomorphic?
\end{quote}
\subsection{11}
\label{sec:org82ba933}
Implementation is straightforward. In the simplest formulation of LLE, there is
only one free parameter: number of neighbors per data point K.

\section{Examples}
\label{sec:orge59e5fc}

Embeddings discovered by LLE are easiest to visualize for data samples from
2-dim manifolds.

Under the right conditions, LLE can learn the stereo-graphic mapping from sphere
to plane.

Figure 5 shows another 2-dim manifold, but one living in a much higher
dimensional space.

Low dimensional outputs of LLE can be used to index the original collection of
high dimensional images. Fast and accurate indexing is an essential component of
example-based video synthesis from a large library of stored frames.

LLE scales relatively well to large datasets because it generates \emph{sparse}
intermediate results and eigenproblems.

\section{Implementation}
\label{sec:orge00e9a0}

The algorithm consists of three steps:
\begin{enumerate}
\item Nearest neighbor search (to identify the non-zero elements of the weight
matrix)
\item Constrained Least Squares Fits (to compute the values of these weights)
\item Singular Value Decomposition (to perform the embedding)
\end{enumerate}

\subsection{Neighborhood Search}
\label{sec:org2e7decc}

In the simplest formulation of the algorithm, one identifies a fixed number of
nearest neighbors, K, per data point, as measured by Euclidean Distance.

The results of LLE are typically stable of a range of neighborhood sizes. The
size of the that range depends on various features of the data, such as the
sampling density and the manifold geometry.

The nearest neighbor step in LLE is simple to implement, though it can be time
consuming for large datasets (\(N \leq 10^4\)) if performed \emph{without} any optimizations.

An implementation of LLE also needs to check that the graph formed by linking
each data point to its neighbors is connected.

\begin{quote}
I am pretty sure this is confirming the assumption of homeomorphic/continuity
within the neighborhood of the points.

Is each neighborhood considered convex?
\end{quote}
\subsection{Constrained Least Squares Fits}
\label{sec:org5cbf4cc}

The second step of LLE is to reconstruct each data point from its nearest
neighbors. The optimal reconstruction weights can be computed in closed form.

\begin{equation}
\begin{split}
\label{eq:3}
\epsilon = |\vec x - \Sigma_j w_j \vec \eta_j|^2 = |\Sigma_j w_j (\vec x - \vec \eta_j)|^2 = \Sigma_{jk} w_j w_k G_{jk}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\label{eq:4}
G_{jk} = (\vec x - \eta_j) \cdot (\vec x - \vec \eta_k)
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\label{eq:5}
w_j = \frac{\Sigma_k G_{jk}^{-1}}{\Sigma_{lm} G_{lm}^{-1}}
\end{split}
\end{equation}



In unusual cases, it can arise that the Gram matrix in \(\eqref{eq:4}\) is
singular or nearly singular. For example, when there are more neighbors than
input dimensions (\(K > D\)), or when the data points are not in general position.

When \(K > D\), Least squares problem for finding the weight does not have a
unique solution. Thus elements of the Gram matrix need to be conditioned before solving.

$$
G_{jk} \leftarrow G_{jk} + \delta_{jk} (\frac{\Delta^2}{K}) Tr(G)
$$

The regularization term (\(\frac{\Delta^2}{K}\)) acts to penalize large weights that exploit correlations
beyond some level of precision in the data sampling process. It may also
introduce some robustness to noise and outliers.

Computing the reconstruction weights \(W_{ij}\) is typically the least expensive
step of the LLE algorithm.

\subsection{Eigenvalue Problem}
\label{sec:org861a824}

The final step of LLE is to compute a low dimensional embedding based on the
reconstruction weights \(W_{ij}\) of the high dimensional inputs \(\vec X_i\). Only
information captured by the weights \(W_{ij}\) is used to construct the embedding.

\begin{equation}
\begin{split}
\label{eq:6}
\Phi(Y) = \Sigma_{ij} M_{ij} (\vec Y_i \cdot \vec Y_j)
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\label{eq:7}
M_{ij} = \delta_{ij} - W_{ij} - W{ji} + \Sigma_k W_{ki} W_{kj}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\label{eq:8}
\Sigma_i \vec Y_i = \vec 0
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\label{eq:9}
\frac{1}{N} \Sigma_i \vec Y_i \vec Y_i^T = I
\end{split}
\end{equation}




The optimization of \(\eqref{eq:6}\) is performed subject to constraints that make
the problem well-posed.

Under these restrictions, the optimal embedding - up to a trivial global
rotation of the embedding space - is found by minimizing \(\eqref{eq:2}\) subject
to the constraints in \(\eqref{eq:8}-\eqref{eq:9}\). This can be done in many
ways, but the most straightforward is to find the bottom \(d + 1\) eigenvectors of
the cost matrix, M. (Bottom or Top eigenvectors correspond to largest or
smallest eigenvalues).

Note that the bottom \(d + 1\) eigenvectors of the sparse, symmetric matrix M can
be found \textbf{without} performing a full matrix diagonalization.

The final step of LLE is typically the most computationally expensive. Without
special optimizations, computing the bottom eigenvectors scales as \(O(dN^2)\).

Note that the \(d^{th}\) coordinate output by LLE always corresponds to the \((d +
1)^{st}\) smallest eigenvector of the matrix M, regardless of the total number
of outputs computed or the order in which they are calculated.

\section{Extensions}
\label{sec:org519ff88}
\subsection{LLE from Pairwise Distances}
\label{sec:org8fb6264}
\end{document}
